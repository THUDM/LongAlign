# LongAlign: A Recipe for Long Context Alignment of LLMs

<p align="center">
    ğŸ¤— <a href="https://huggingface.co/datasets/THUDM/LongAlign-10k" target="_blank">HF ä»“åº“</a> â€¢ ğŸ“ƒ <a href="https://arxiv.org/abs/2401.18058" target="_blank">è®ºæ–‡</a>
</p>

Read this in [English](README.md).

**LongAlign** æ˜¯é¦–ä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é’ˆå¯¹é•¿ä¸Šä¸‹æ–‡å¯¹é½çš„å…¨é¢æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº† **LongAlign-10k** æ•°æ®é›†ï¼ŒåŒ…å« 10,000 æ¡é•¿æŒ‡ä»¤æ•°æ®ï¼Œé•¿åº¦åœ¨ 8k-64k ä¹‹é—´ã€‚æˆ‘ä»¬ç ”ç©¶äº†è®­ç»ƒç­–ç•¥ï¼Œå³ **packing (with loss weighting)** å’Œ **sorted batching**ï¼Œè¿™äº›éƒ½å®ç°åœ¨äº†æˆ‘ä»¬çš„ä»£ç ä¸­ã€‚ä¸ºäº†è¯„ä¼°çœŸå®ä¸–ç•Œä¸­æ¨¡å‹é•¿ä¸Šä¸‹æ–‡çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº† **LongBench-Chat**ï¼Œå®ƒèƒ½å¤Ÿè¯„ä¼°LLMå¯¹ 10k-100k é•¿åº¦ä»»åŠ¡çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚

## ğŸ” ç›®å½•
- [âš™ï¸ æ•°æ®å‡†å¤‡](#data-preparation)
- [ğŸ–¥ï¸ LongAlign è®­ç»ƒ](#longalign-training)
- [ğŸ“Š è¯„æµ‹](#longbench-chat-evaluation)
- [ğŸ“ å¼•ç”¨](#citation)

<a name="data-preparation"></a>

## âš™ï¸ æ•°æ®å‡†å¤‡

æ‚¨å¯ä»¥é€šè¿‡ Hugging Face datasets ä¸‹è½½å¹¶ä¿å­˜ **LongAlign-10k** æ•°æ®ï¼ˆ[ğŸ¤— HF ä»“åº“](https://huggingface.co/datasets/THUDM/LongAlign-10k)ï¼‰ï¼š
```python
dataset = load_dataset('THUDM/LongAlign-10k')
for split, split_dataset in dataset.items():
    split_dataset.to_json("data/raw/long.jsonl")
```
ShareGPT æ•°æ®å¯ä»¥ä»[è¿™é‡Œ](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main/HTML_cleaned_raw_dataset)ä¸‹è½½ã€‚æˆ‘ä»¬å‚è€ƒäº† [open-instruct](https://github.com/allenai/open-instruct) ä»“åº“å¤„ç† ShareGPT æ•°æ®ã€‚è¯·å°†æ•°æ®æ–‡ä»¶ä¿å­˜åœ¨ `data/raw/sharegpt.jsonl`ã€‚æ‚¨å¯ä»¥ä½¿ç”¨å…¶ä»–æ•°æ®ä½œä¸ºä¸€èˆ¬æŒ‡ä»¤æ•°æ®çš„æ¥æºï¼Œä½†è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å¤„ç†å¥½æ‚¨çš„æ•°æ®ï¼š
```json
{
    "messages": [{"role": "user", "content": "..."}, 
                 {"role": "assistant", "content": "..."}, ...]
    }
```

<a name="longalign-training"></a>
## ğŸ–¥ï¸ LongAlign è®­ç»ƒ

### ç¯å¢ƒè®¾ç½®
ä½¿ç”¨ pip å®‰è£…æ‰€éœ€ç¯å¢ƒï¼š`pip install -r requirements.txt`ã€‚å¯¹äºåŸºäº Llama çš„æ¨¡å‹ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ FlashAttention 2 è¿›è¡Œä¼˜åŒ–å¹¶èŠ‚çœ GPU å†…å­˜ã€‚ç›¸å…³ä¾èµ–é¡¹å¯ä»¥æ ¹æ® [FlashAttention](https://github.com/Dao-AILab/flash-attention) çš„ä»£ç åº“è¿›è¡Œå®‰è£…ã€‚

### æ•°æ®é¢„å¤„ç†

é¦–å…ˆï¼Œä½¿ç”¨æ¨¡å‹çš„åˆ†è¯å™¨å¯¹åŸå§‹æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚ä¾‹å¦‚ï¼Œè®­ç»ƒ ChatGLM æ—¶ï¼š
```bash
python pre_tokenize.py --model chatglm --datanum 10k
```
è¿™é‡Œçš„ `--datanum` å‚æ•°æŒ‡çš„æ˜¯æ‚¨æƒ³è¦åœ¨è®­ç»ƒæ•°æ®é›†ä¸­æ··å…¥çš„é•¿æ•°æ®çš„æ•°æ®é‡ï¼ˆæˆ‘ä»¬çš„è®ºæ–‡ç ”ç©¶äº† 0kã€5k å’Œ 10kï¼‰ã€‚åˆ†è¯åçš„æ•°æ®å°†ä¿å­˜åœ¨ `./data/chatglm/10k` ä¸‹ã€‚

å¯¹äº packing å’Œ sorted batching ç­–ç•¥ï¼Œæˆ‘ä»¬å°†æ•°æ®å¤„ç†æˆè®­ç»ƒæ‰€éœ€æ ¼å¼ï¼š
```bash
python sort_and_group.py --group_size 8 --train_file ./data/chatglm/10k
```
æ‚¨åº”è¯¥å°† `--group_size` å‚æ•°è®¾ç½®ä¸ºè®­ç»ƒæœŸé—´çš„ GPU æ•°é‡ã€‚æˆ‘ä»¬å»ºè®®è‡³å°‘ä½¿ç”¨ 8 ä¸ª 80G GPU è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå¦åˆ™ 64k é•¿åº¦å¯èƒ½ä¼šå¯¼è‡´å†…å­˜æº¢å‡ºã€‚

### æ¨¡å‹è®­ç»ƒ

æˆ‘ä»¬åœ¨ `scripts/` ä¸‹æä¾›äº† ChatGLM3 å’Œ Llama-2 æ¨¡å‹ç³»åˆ—çš„è®­ç»ƒè„šæœ¬ã€‚è¯·è°ƒæ•´ `--model_name_or_path`ã€`--train_file` å’Œ `--output_dir` ä»¥åŒ¹é…æ‚¨çš„æ¨¡å‹è·¯å¾„ã€æ•°æ®è·¯å¾„å’Œè¾“å‡ºè·¯å¾„ã€‚è¯·ä½¿ç”¨è‡³å°‘æœ‰ 64k ä¸Šä¸‹æ–‡çª—å£é•¿åº¦çš„åŸºåº§æ¨¡å‹ã€‚æˆ‘ä»¬å‘å¸ƒäº†ä¸‰ä¸ª **åŸºåº§æ¨¡å‹**ï¼Œä¸Šä¸‹æ–‡çª—å£æ‰©å±•åˆ° 64kï¼š[LongAlign-6B-64k-base](https://huggingface.co/THUDM/LongAlign-6B-64k-base)ã€[LongAlign-7B-64k-base](https://huggingface.co/THUDM/LongAlign-7B-64k-base) å’Œ [LongAlign-13B-64k-base](https://huggingface.co/THUDM/LongAlign-13B-64k-base)ã€‚

å¯¹äº packing è®­ç»ƒï¼Œè¯·ä¿®æ”¹*æ³¨æ„åŠ›è®¡ç®—*ä»¥æ”¯æŒä¼ å…¥æ ‡è®°äº†æ¯ä¸ªåºåˆ—åœ¨ pack ä¸­èµ·æ­¢ä½ç½®çš„ 1D æ³¨æ„åŠ›æ©ç ï¼Œä»¥åŠ*æ¨¡å‹å‰å‘è®¡ç®—*å‡½æ•°ä»¥æ”¯æŒ loss weightingã€‚æˆ‘ä»¬ä¸º ChatGLM3 æ¨¡å‹æä¾›äº†æ­¤ç±»ä¿®æ”¹çš„ç¤ºä¾‹ï¼Œåœ¨ [modeling_chatglm.py](https://github.com/THUDM/LongAlign/blob/main/modeling_chatglm.py) ä¸­çš„ `CoreAttention.forward` å’Œ `ChatGLMForConditionalGeneration.forward`ã€‚æ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨æ­¤æ–‡ä»¶ä½œä¸º ChatGLM è®­ç»ƒä¸­çš„æ¨¡å‹æ–‡ä»¶ã€‚æˆ‘ä»¬ä¹Ÿæä¾›äº† Llama çš„è®­ç»ƒä»£ç ï¼Œå¦‚æœè¦å¤ç°æˆ‘ä»¬çš„ç»“æœï¼Œè¯·ä½¿æœ¬ Repo ä¸­çš„ [modeling_llama.py](https://github.com/THUDM/LongAlign/blob/main/modeling_llama.py) ä½œä¸ºæ¨¡å‹æ–‡ä»¶ã€‚æ ¹æ®æˆ‘ä»¬è®ºæ–‡ä¸­çš„å®éªŒç»“æœï¼Œæˆ‘ä»¬æ¨èå¯¹ ChatGLM è®­ç»ƒä½¿ç”¨ *packing+loss weighting*ï¼Œå¯¹ Llama è®­ç»ƒä½¿ç”¨ *sorted batching*ã€‚

### æ¨¡å‹éƒ¨ç½²
æˆ‘ä»¬å‘å¸ƒäº†å››ä¸ªä½¿ç”¨ LongAlign è®­ç»ƒçš„ **chat æ¨¡å‹**ï¼š[LongAlign-6B-64k](https://huggingface.co/THUDM/LongAlign-6B-64k)ï¼ˆåŸºäº *ChatGLM3-6B*ï¼‰ã€[LongAlign-7B-64k](https://huggingface.co/THUDM/LongAlign-7B-64k)ï¼ˆåŸºäº *Llama-2-7B*ï¼‰ã€[LongAlign-13B-64k](https://huggingface.co/THUDM/LongAlign-13B-64k)ï¼ˆåŸºäº *Llama-2-13B*ï¼‰å’Œ [ChatGLM3-6B-128k](https://huggingface.co/THUDM/chatglm3-6b-128k)ã€‚æ‚¨å¯ä»¥ç”¨è¿™ä¸ª demo ä»£ç æ¥å°è¯•ä½¿ç”¨æ¨¡å‹æ¥æ€»ç»“æˆ‘ä»¬çš„è®ºæ–‡ï¼Œæˆ–è¯¢é—®æœ‰å…³çš„ä»»ä½•é—®é¢˜ï¼š
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
tokenizer = AutoTokenizer.from_pretrained("THUDM/LongAlign-6B-64k", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("THUDM/LongAlign-6B-64k", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map="auto")
model = model.eval()
query = open("assets/paper.txt").read() + "\n\nè¯·æ€»ç»“è¿™ç¯‡è®ºæ–‡ã€‚"
response, history = model.chat(tokenizer, query, history=[], max_new_tokens=512, temperature=1)
print(response)
```
å¯¹äºåŸºäº Llama çš„æ¨¡å‹ï¼Œæˆ‘ä»¬è¿˜æä¾›äº† [llama_flash_attn_monkey_patch.py](https://github.com/THUDM/LongAlign/blob/main/LongBench_Chat/llama_flash_attn_monkey_patch.py)ï¼Œä»¥ä¾¿åœ¨é•¿åºåˆ—æ¨ç†æ—¶åˆ©ç”¨ FlashAttention-2 ä»¥èŠ‚çœæ˜¾å­˜ã€‚

### æ‰€æœ‰å¯ç”¨æ¨¡å‹

è¿™é‡Œæ˜¯æˆ‘ä»¬å‘å¸ƒçš„å¼€æºæ¨¡å‹çš„å®Œæ•´åˆ—è¡¨ï¼š

| æ¨¡å‹                      | HF ä»“åº“                                                      | æè¿°                                                     |
| ------------------------- | ------------------------------------------------------------ | -------------------------------------------------------- |
| **LongAlign-6B-64k-base** | [ğŸ¤— HF ä»“åº“](https://huggingface.co/THUDM/LongAlign-6B-64k-base) | **ChatGLM3-6B** ä¸Šä¸‹æ–‡çª—å£æ‰©å±•åˆ° 64k                     |
| **LongAlign-6B-64k**      | [ğŸ¤— HF ä»“åº“](https://huggingface.co/THUDM/LongAlign-6B-64k)   | åŸºäº LongAlign åœ¨ LongAlign-6B-64k-base ä¸Šè®­ç»ƒçš„ chat æ¨¡å‹ |
| **LongAlign-7B-64k-base** | [ğŸ¤— HF ä»“åº“](https://huggingface.co/THUDM/LongAlign-7B-64k-base) | **Llama-2-7B** ä¸Šä¸‹æ–‡çª—å£æ‰©å±•åˆ° 64k                      |
|**LongAlign-7B-64k**| [ğŸ¤— HF ä»“åº“](https://huggingface.co/THUDM/LongAlign-7B-64k) | åŸºäº LongAlign åœ¨ LongAlign-7B-64k-base ä¸Šè®­ç»ƒçš„ chat æ¨¡å‹ |
|**LongAlign-13B-64k-base**| [ğŸ¤— HF ä»“åº“](https://huggingface.co/THUDM/LongAlign-13B-64k-base) | **Llama-2-13B** ä¸Šä¸‹æ–‡çª—å£æ‰©å±•åˆ° 64k |
|**LongAlign-13B-64k**| [ğŸ¤— HF ä»“åº“](https://huggingface.co/THUDM/LongAlign-13B-64k) | åŸºäº LongAlign åœ¨ LongAlign-13B-64k-base ä¸Šè®­ç»ƒçš„ chat æ¨¡å‹ |
|**ChatGLM3-6B-128k**| [ğŸ¤— HF ä»“åº“](https://huggingface.co/THUDM/chatglm3-6b-128k) | **ChatGLM3-6B** ä¸Šä¸‹æ–‡çª—å£æ‰©å±•åˆ° 128k|

<a name="longbench-chat-evaluation"></a>
## ğŸ“Š è¯„æµ‹

### LongBench-Chat è¯„æµ‹
LongBench-Chat æ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°é•¿ä¸Šä¸‹æ–‡å¯¹é½çš„åŸºå‡†æµ‹è¯•ï¼Œé—®é¢˜éƒ½æ¥æºäºçœŸå®ç”¨æˆ·æé—®ï¼Œæµ‹è¯•æ•°æ®é•¿åº¦åœ¨ 10k-100k ä¹‹é—´ã€‚æ•°æ®é›†å’Œè¯„ä¼°ä»£ç åœ¨ `LongBench_Chat/` ä¸‹ã€‚è®°å¾—åœ¨ `eval.py` ä¸­é…ç½®æ‚¨çš„ OpenAI API å¯†é’¥ï¼Œå› ä¸ºæˆ‘ä»¬é‡‡ç”¨ GPT-4 ä½œä¸ºè¯„ä¼°å™¨ã€‚è¿è¡Œ
```bash
python eval.py --model {model_path} --max_length {max_length}
```
`model_path` å¯ä»¥æ˜¯æ‚¨çš„æœ¬åœ°æ¨¡å‹è·¯å¾„æˆ– Hugging Face æ¨¡å‹è·¯å¾„ã€‚è¿™æ˜¯ LongBench-Chat ä¸Šçš„æ’è¡Œæ¦œï¼š

![](assets/leaderboard.png)

æˆ‘ä»¬ä¹Ÿæ¬¢è¿æ‚¨æäº¤æ‚¨çš„æ¨¡å‹é¢„æµ‹ç»“æœæˆ–æµ‹è¯•ç»“æœã€‚æˆ‘ä»¬åœ¨è®¡åˆ’å‘å¸ƒä¸€ä¸ªæ›´åŠ æ­£å¼çš„æ’è¡Œæ¦œã€‚

### å¤§æµ·æé’ˆè¯•éªŒè¯„æµ‹
æˆ‘ä»¬è¿˜æä¾›äº†åœ¨â€œå¤§æµ·æé’ˆâ€æµ‹è¯•ä¸‹è¯„ä¼°HuggingFaceæ¨¡å‹çš„ä»£ç ï¼Œä½äº`Needle_test/`ç›®å½•ä¸‹ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…å…¶ [README.md](https://github.com/THUDM/LongAlign/blob/main/Needle_test/README.md)ã€‚

*ä¸ºäº†å¤ç°æˆ‘ä»¬åœ¨å…¶ä»–åŸºå‡†æµ‹è¯•ä¸Šçš„ç»“æœï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ [LongBench](https://github.com/THUDM/LongBench)ã€[FastChat](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge) å’Œ [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) ä¸­çš„ä»£ç æ¥è¯„ä¼° LongBenchã€MT-Bench å’Œ Open LLM Leaderboard ä¸­çš„ä»»åŠ¡ã€‚*

<a name="citation"></a>
## ğŸ“ å¼•ç”¨

å¦‚æœæ‚¨è®¤ä¸ºæˆ‘ä»¬çš„å·¥ä½œæœ‰ç”¨ï¼Œè¯·è€ƒè™‘å¼•ç”¨ LongAlignï¼š

```
@article{bai2024longalign,
  title={LongAlign: A Recipe for Long Context Alignment of Large Language Models},
  author={Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, Juanzi Li},
  journal={arXiv preprint arXiv:2401.18058},
  year={2024}
}
```
